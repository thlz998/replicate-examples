#!/usr/bin/env python3
import sys
from huggingface_hub import login
import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, StableDiffusionPipeline, EulerDiscreteScheduler
# from lora_diffusion import LoRAManager
from PIL import Image

CACHE_DIR = "weights-cache"
sys.path.append(".")
qrImage = Image.open('k0a1a.png')

seed = 1383557390

model_key_base_safetensors = "./weights-cache/models/revAnimated_v122.safetensors"
print("11111----------------------------")
pipe = StableDiffusionPipeline.from_single_file(
        model_key_base_safetensors,
        torch_dtype=torch.float16,
        cache_dir=CACHE_DIR).to("cuda")
print("22222----------------------------")

components = pipe.components
print("33333----------------------------")

controlnet_qr_pattern = ControlNetModel.from_pretrained(
        "Nacholmo/controlnet-qr-pattern",  
        torch_dtype=torch.float16,
        force_download=False,
        cache_dir=CACHE_DIR).to("cuda")
print("44444----------------------------")

controlnet_qr_monster = ControlNetModel.from_pretrained(
        "monster-labs/control_v1p_sd15_qrcode_monster", 
        torch_dtype=torch.float16,
        force_download=False,
        cache_dir=CACHE_DIR).to("cuda")
print("55555----------------------------")

components["controlnet"] = [controlnet_qr_monster, controlnet_qr_pattern]
# vae (FlaxAutoencoderKL) — 变分自编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。
# text_encoder (FlaxCLIPTextModel) — 冻结的文本编码器。Stable Diffusion使用CLIP的文本部分，特别是clip-vit-large-patch14变体。
# tokenizer (CLIPTokenizer) — 类CLIPTokenizer的标记器。
# unet (FlaxUNet2DConditionModel) — 有条件U-Net架构，用于去噪编码后的图像潜在表示形式。
# controlnet (FlaxControlNetModel）— 在去噪过程中为unet提供额外的调节能力。
# scheduler (SchedulerMixin) — 调度程序与unet一起使用以去噪编码后的图像潜在表示形式。可以是以下之一： FlaxDDIMScheduler、FlaxLMSDiscreteScheduler、FlaxPNDMScheduler或者 FlaxDPMSolverMultistepScheduler.
# safety_checker(FlaxStableDiffusionSafetyChecker)—分类模块，估计生成图片是否可能被认为具有冒犯性或有害性。请参阅模型卡片了解详情。
# feature_extractor(CLIPFeatureExtractor)-从生成图片中提取特征并作为safety_checker输入使用的模型。

pipe = StableDiffusionControlNetPipeline(**components).to("cuda")
print("66666----------------------------")

lora_model_file = './weights-cache/Lora/blindbox_v1_mix.safetensors'
pipe.load_lora_weights(lora_model_file, cache_dir=CACHE_DIR)
print("77777----------------------------")

pipe = pipe.to("cuda", torch.float16)

device = 'cuda'
generator = torch.Generator(device=device).manual_seed(seed)

prompt = '(masterpiece),(best quality),(ultra-detailed), (full body:1.2),1girl,chibi,cute, smile, open mouth,flower, outdoors, playing guitar, music, beret, holding guitar, jacket, blush, tree, :3, shirt, short hair, cherry blossoms, green headwear, blurry, brown hair, blush stickers, long sleeves, bangs, headphones, black hair, pink flower,(beautiful detailed face), (beautiful detailed eyes)'
negative_prompt = '(low quality:1.3), (worst quality:1.3)'
guidance_scale = 7
num_inference_steps = 35

scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)
pipe.scheduler = scheduler
pipe.scheduler.set_timesteps(num_inference_steps)

images = pipe(
prompt=prompt, 
negative_prompt=negative_prompt,
guidance_scale=guidance_scale,
num_inference_steps=num_inference_steps,
generator=generator,
image=[qrImage, qrImage],
controlnet_conditioning_scale=[1.0, 1.0],
control_guidance_start=[0, 0],
control_guidance_end=[1, 1]
).images[0]

images.save(f'output.png')